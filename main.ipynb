{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48184bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kennym1/.conda/envs/MA384/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c946e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b980bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      name  size_bytes suffix date_modified  \\\n",
      "0  +----------------------------------.txt        3112   .txt    2025-07-10   \n",
      "1                                .Rhistory        1297           2025-12-22   \n",
      "2                            ._ipp2-source         178           2022-09-07   \n",
      "3      01 PCProcessorMicroarchitecture.pdf      170784   .pdf    2025-03-13   \n",
      "4               01-probability-review.docx       19858  .docx    2025-12-09   \n",
      "\n",
      "  date_created  \n",
      "0   2025-07-10  \n",
      "1   2025-12-22  \n",
      "2   2025-11-04  \n",
      "3   2025-03-13  \n",
      "4   2025-12-09  \n"
     ]
    }
   ],
   "source": [
    "# # Path to Downloads (Windows example)\n",
    "# downloads_path = Path.home() / \"Downloads\"\n",
    "\n",
    "# # Collect file info\n",
    "# data = []\n",
    "# for file in downloads_path.iterdir():\n",
    "#     if file.is_file():\n",
    "#         stat = file.stat()\n",
    "#         data.append({\n",
    "#             \"name\": file.name,\n",
    "#             \"size_bytes\": stat.st_size,\n",
    "#             \"suffix\": file.suffix,\n",
    "#             \"date_modified\": pd.to_datetime(stat.st_mtime, unit=\"s\").strftime(\"%Y-%m-%d\"),\n",
    "#             \"date_created\": pd.to_datetime(stat.st_birthtime, unit=\"s\").strftime(\"%Y-%m-%d\"),\n",
    "#         })\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c253d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      name  size_bytes suffix date_modified  \\\n",
      "0  +----------------------------------.txt        3112   .txt    2025-07-10   \n",
      "1                                .Rhistory        1297    NaN    2025-12-22   \n",
      "2                            ._ipp2-source         178    NaN    2022-09-07   \n",
      "3      01 PCProcessorMicroarchitecture.pdf      170784   .pdf    2025-03-13   \n",
      "4               01-probability-review.docx       19858  .docx    2025-12-09   \n",
      "\n",
      "  date_created  \n",
      "0   2025-07-10  \n",
      "1   2025-12-22  \n",
      "2   2025-11-04  \n",
      "3   2025-03-13  \n",
      "4   2025-12-09  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/kennym1/download_analysis/downloads_info.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"downloads_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa446e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix\n",
      ".pdf                                        570\n",
      ".docx                                        68\n",
      ".pptx                                        67\n",
      ".png                                         45\n",
      ".html                                        37\n",
      ".txt                                         26\n",
      ".csv                                         22\n",
      ".sql                                         17\n",
      ".jpg                                         14\n",
      ".jfif                                        11\n",
      ".xlsx                                        10\n",
      ".bmpr                                         9\n",
      ".jpeg                                         8\n",
      ".json                                         6\n",
      ".ans                                          6\n",
      ".in                                           6\n",
      ".py                                           6\n",
      ".v                                            5\n",
      ".java                                         5\n",
      ".ipynb                                        4\n",
      ".log                                          4\n",
      ".c                                            4\n",
      ".tex                                          4\n",
      ".ics                                          3\n",
      ".wav                                          3\n",
      ".js                                           3\n",
      ".rtf                                          2\n",
      ".ini                                          2\n",
      ".doc                                          2\n",
      ".mp4                                          2\n",
      ".iso                                          2\n",
      ".url                                          1\n",
      ".cr_3da527c9-a1c1-4e7b-b6e3-697ca37d4873      1\n",
      ".schem                                        1\n",
      ".tsv                                          1\n",
      ".appinstaller                                 1\n",
      ".lnk                                          1\n",
      ".vsdx                                         1\n",
      ".sof                                          1\n",
      ".vbs                                          1\n",
      ".mw                                           1\n",
      ".cpp                                          1\n",
      ".yankai                                       1\n",
      ".chm                                          1\n",
      ".puml                                         1\n",
      ".UF2                                          1\n",
      ".msix                                         1\n",
      ".env                                          1\n",
      ".whl                                          1\n",
      ".aux                                          1\n",
      ".fdb_latexmk                                  1\n",
      ".fls                                          1\n",
      ".out                                          1\n",
      ".cns                                          1\n",
      ".bak                                          1\n",
      ".es3                                          1\n",
      ".bac                                          1\n",
      ".properties                                   1\n",
      ".do                                           1\n",
      ".mp3                                          1\n",
      ".bibtex                                       1\n",
      ".eml                                          1\n",
      ".latex                                        1\n",
      ".msi                                          1\n",
      ".ogg                                          1\n",
      ".xz                                           1\n",
      ".tmp                                          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.suffix.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 1: Rule-based by extension (instant, no model needed) ---\n",
    "EXTENSION_RULES = {\n",
    "    # Software\n",
    "    \".exe\": \"Software Installer\", \".msi\": \"Software Installer\",\n",
    "    \".iso\": \"Software Installer\", \".dmg\": \"Software Installer\",\n",
    "    \".apk\": \"Software Installer\", \".deb\": \"Software Installer\",\n",
    "    \".msix\": \"Software Installer\", \".appinstaller\": \"Software Installer\",\n",
    "    \".whl\": \"Software Installer\",\n",
    "    # Media\n",
    "    \".mp4\": \"Media or Entertainment\", \".mkv\": \"Media or Entertainment\",\n",
    "    \".avi\": \"Media or Entertainment\", \".mp3\": \"Media or Entertainment\",\n",
    "    \".wav\": \"Media or Entertainment\", \".flac\": \"Media or Entertainment\",\n",
    "    \".ogg\": \"Media or Entertainment\",\n",
    "    # Images\n",
    "    \".jpg\": \"Photo or Image\", \".jpeg\": \"Photo or Image\",\n",
    "    \".jfif\": \"Photo or Image\",\n",
    "    \".png\": \"Photo or Image\",  \".gif\": \"Photo or Image\",\n",
    "    \".svg\": \"Photo or Image\",  \".psd\": \"Creative Project\",\n",
    "    # Code / Data\n",
    "    \".py\": \"Dataset or Code\", \".js\": \"Dataset or Code\",\n",
    "    \".csv\": \"Dataset or Code\", \".json\": \"Dataset or Code\",\n",
    "    \".ipynb\": \"Dataset or Code\", \".sql\": \"Dataset or Code\",\n",
    "    \".c\": \"Dataset or Code\", \".cpp\": \"Dataset or Code\",\n",
    "    \".java\": \"Dataset or Code\", \".v\": \"Dataset or Code\",\n",
    "    \".tsv\": \"Dataset or Code\",\n",
    "    # Archives\n",
    "    \".zip\": \"Archive or Backup\", \".rar\": \"Archive or Backup\",\n",
    "    \".tar\": \"Archive or Backup\", \".gz\": \"Archive or Backup\",\n",
    "    \".7z\": \"Archive or Backup\", \".xz\": \"Archive or Backup\",\n",
    "    \".bak\": \"Archive or Backup\", \".bac\": \"Archive or Backup\",\n",
    "    \".tmp\": \"Archive or Backup\",\n",
    "}\n",
    "\n",
    "# --- Stage 2: Keyword rules applied to the cleaned filename stem ---\n",
    "# More keywords = fewer files need the slow NLP model\n",
    "KEYWORD_RULES = {\n",
    "    \"Schoolwork\": [\n",
    "        \"lecture\", \"homework\", \"assignment\", \"exam\", \"quiz\", \"lab\",\n",
    "        \"midterm\", \"final\", \"chapter\", \"tutorial\", \"worksheet\", \"syllabus\",\n",
    "        \"notes\", \"slide\", \"textbook\", \"problem set\", \"pset\",\n",
    "        # common CS/stats course topics\n",
    "        \"probability\", \"statistics\", \"algorithm\", \"microarchitecture\",\n",
    "        \"hypothesis\", \"estimation\", \"likelihood\", \"regression\", \"prediction\",\n",
    "        \"processor\", \"architecture\", \"operating system\", \"compiler\",\n",
    "        \"database\", \"network\", \"machine learning\", \"data structure\",\n",
    "        \"sorting\", \"search\", \"graph\", \"tree\", \"recursion\",\n",
    "        \"prior\", \"posterior\", \"bayesian\",\n",
    "    ],\n",
    "    \"Research Paper\": [\n",
    "        \"survey\", \"review\", \"study\", \"analysis\", \"evaluation\",\n",
    "        \"proceedings\", \"conference\", \"acm\", \"ieee\", \"arxiv\",\n",
    "        \"journal\", \"abstract\", \"methodology\", \"experiment\",\n",
    "    ],\n",
    "    \"Financial\": [\n",
    "        \"invoice\", \"receipt\", \"bank\", \"statement\", \"tax\", \"budget\",\n",
    "        \"payroll\", \"expense\", \"payment\", \"billing\", \"account\",\n",
    "    ],\n",
    "    \"Personal Documentation\": [\n",
    "        \"resume\", \"cv\", \"cover letter\", \"passport\", \"license\",\n",
    "        \"certificate\", \"transcript\", \"contract\", \"agreement\", \"lease\",\n",
    "        \"insurance\", \"medical\", \"prescription\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- Stage 3: NLP zero-shot for files that pass through keyword stage ---\n",
    "AMBIGUOUS_EXTENSIONS = {\".pdf\", \".docx\", \".doc\", \".txt\", \".pptx\", \".xlsx\",\n",
    "                        \".xls\", \".html\", \".rtf\", \".tex\", \".latex\", \".md\"}\n",
    "\n",
    "NLP_LABELS = [\n",
    "    \"Personal Documentation\",\n",
    "    \"Schoolwork or Course Material\",\n",
    "    \"Financial Record\",\n",
    "    \"Research Paper or Academic Article\",\n",
    "    \"Creative Project\",\n",
    "    \"Dataset or Code\",\n",
    "    \"Archive or Backup\",\n",
    "]\n",
    "\n",
    "\n",
    "def clean_for_nlp(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Produce a human-readable description from a raw filename so the\n",
    "    zero-shot model has something meaningful to classify.\n",
    "    \"\"\"\n",
    "    stem = Path(name).stem\n",
    "    # Strip leading numeric prefix: \"01 \", \"02-\", \"03. \", etc.\n",
    "    stem = re.sub(r'^\\d+[\\s.\\-_]+', '', stem)\n",
    "    # Strip UUID-like tokens (they add noise, not signal)\n",
    "    stem = re.sub(r'\\b[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\\b', '', stem, flags=re.I)\n",
    "    # Split CamelCase: \"PCProcessorMicroarchitecture\" → \"PC Processor Microarchitecture\"\n",
    "    stem = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', stem)\n",
    "    stem = re.sub(r'([A-Z]+)([A-Z][a-z])', r'\\1 \\2', stem)\n",
    "    # Replace common separators with spaces\n",
    "    stem = stem.replace('_', ' ').replace('-', ' ').replace('.', ' ')\n",
    "    # Collapse multiple spaces\n",
    "    stem = re.sub(r'\\s+', ' ', stem).strip()\n",
    "    return stem\n",
    "\n",
    "\n",
    "def keyword_classify(cleaned_stem: str) -> str | None:\n",
    "    \"\"\"Return a label if any keyword matches, else None (needs NLP).\"\"\"\n",
    "    lower = cleaned_stem.lower()\n",
    "    for label, keywords in KEYWORD_RULES.items():\n",
    "        if any(kw in lower for kw in keywords):\n",
    "            return label\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d302adde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 106/106 [00:00<00:00, 368.18it/s, Materializing param=pooler.dense.weight]                                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/nli-deberta-v3-small\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "deberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=\"cross-encoder/nli-deberta-v3-small\",\n",
    "        device=0,  # remove this line if you don't have a GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_rules(row) -> tuple[str | None, str]:\n",
    "    \"\"\"Returns (category, cleaned_name). category is None if NLP is needed.\"\"\"\n",
    "    ext = str(row[\"suffix\"]).lower() if pd.notna(row[\"suffix\"]) else \"\"\n",
    "    name = str(row[\"name\"])\n",
    "    cleaned = clean_for_nlp(name)\n",
    "\n",
    "    # Stage 1: hard extension rules\n",
    "    if ext in EXTENSION_RULES:\n",
    "        return EXTENSION_RULES[ext], cleaned\n",
    "\n",
    "    # Stage 2: keyword match on cleaned stem (fast, no model)\n",
    "    if ext in AMBIGUOUS_EXTENSIONS:\n",
    "        kw_label = keyword_classify(cleaned)\n",
    "        if kw_label:\n",
    "            return kw_label, cleaned\n",
    "        return None, cleaned  # falls through to NLP\n",
    "\n",
    "    return \"Other\", cleaned\n",
    "\n",
    "results_tuple = df.apply(classify_with_rules, axis=1, result_type=\"expand\")\n",
    "df[\"category\"] = results_tuple[0]\n",
    "df[\"cleaned_name\"] = results_tuple[1]\n",
    "\n",
    "needs_nlp = df[\"category\"].isna()\n",
    "print(f\"Extension rules : {(df['category'].notna() & df['category'].ne('Other')).sum()} files\")\n",
    "print(f\"Keyword rules   : {needs_nlp.sum()} still need NLP\")\n",
    "print(f\"Total needing NLP: {needs_nlp.sum()} / {len(df)}\")\n",
    "\n",
    "if needs_nlp.any():\n",
    "    texts = df.loc[needs_nlp, \"cleaned_name\"].tolist()\n",
    "\n",
    "    nlp_results = classifier(\n",
    "        texts,\n",
    "        candidate_labels=NLP_LABELS,\n",
    "        # More natural framing for document filenames — avoids the generic\n",
    "        # \"This example is X\" default which inflates Archive/Backup scores\n",
    "        hypothesis_template=\"This file is a {}.\",\n",
    "        batch_size=16,\n",
    "    )\n",
    "\n",
    "    df.loc[needs_nlp, \"category\"] = [r[\"labels\"][0] for r in nlp_results]\n",
    "\n",
    "print(\"\\nSample results:\")\n",
    "print(df[[\"name\", \"suffix\", \"cleaned_name\", \"category\"]].head(25).to_string(index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4168a237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MA384",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
